{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "from attention import AttentionLayer\n",
    "from DataPreprocessing import DataPreprocessing\n",
    "from TextCleaner import TextCleaner\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = DataPreprocessing()\n",
    "cleaner = TextCleaner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pickled Data\n",
    "data =  processor.load_pickle('DataSequences')\n",
    "\n",
    "x_tr, x_test, x_dev, y_tr, y_test, y_dev = data[0],data[1],data[2],data[3],data[4],data[5]\n",
    "\n",
    "loaded_data = processor.load_pickle('TokenizerData')\n",
    "\n",
    "x_tokenizer, y_tokenizer, x_vocab_size,y_vocab_size, input_word_index,target_word_index, reversed_input_word_index, reversed_target_word_index, max_length_text, max_length_summary = loaded_data[0],loaded_data[1], loaded_data[2],loaded_data[3],loaded_data[4],loaded_data[5],loaded_data[6],loaded_data[7],loaded_data[8],loaded_data[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Summarizer(): \n",
    "    def __init__(self):\n",
    "        self.latent_dim = 128\n",
    "        self.embedding_dim = 100\n",
    "        \n",
    "    def read_glove_embeddings(self):\n",
    "        embeddings_index = dict()\n",
    "        f = open('glove.6B.100d.txt',encoding = 'utf8')\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "        f.close()\n",
    "        \n",
    "        return embeddings_index\n",
    "    \n",
    "    def embedding_matrix(self, embeddings_index):\n",
    "        embedding_matrix_input = np.zeros((x_vocab_size,100))\n",
    "\n",
    "        for word, idx in input_word_index.items():\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix_input[idx,:] = embedding_vector\n",
    "            else:\n",
    "                new_embedding = np.random.uniform(-1,1,(1,100))\n",
    "                embedding_matrix_input[idx,:] = new_embedding\n",
    "                \n",
    "        embedding_matrix_target = np.zeros((y_vocab_size, 100))\n",
    "\n",
    "        for word, idx in target_word_index.items():\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix_target[idx,:] = embedding_vector\n",
    "            else:\n",
    "                new_embedding = np.random.uniform(-1,1,(1,100))\n",
    "                embedding_matrix_target[idx,:] = new_embedding\n",
    "                \n",
    "        return embedding_matrix_input, embedding_matrix_target\n",
    "    \n",
    "    \n",
    "    def define_models(self, embedding_matrix_input, embedding_matrix_target):\n",
    "        \"\"\"Training Phase\"\"\"\n",
    "        # Encoder\n",
    "        encoder_inputs = Input(shape = (max_length_text,))\n",
    "        enc_emb = Embedding(x_vocab_size,self.embedding_dim,weights=[embedding_matrix_input],input_length=max_length_text,trainable=False)(encoder_inputs)\n",
    "        encoder = Bidirectional(LSTM(self.latent_dim, return_sequences=True,return_state=True,dropout=0.3,recurrent_dropout=0.3))\n",
    "        encoder_outputs,forward_h,forward_c,backward_h,backward_c = encoder(enc_emb)\n",
    "        state_h = Concatenate()([forward_h,backward_h])\n",
    "        state_c = Concatenate()([forward_c,backward_c])\n",
    "        \n",
    "        # Decoder\n",
    "        decoder_inputs = Input(shape=(None,))\n",
    "        dec_emb_layer = Embedding(y_vocab_size,self.embedding_dim,weights=[embedding_matrix_target],input_length=max_length_summary,trainable=False)\n",
    "        dec_emb = dec_emb_layer(decoder_inputs)\n",
    "        decoder_lstm = LSTM(2*self.latent_dim, return_sequences=True,return_state=True,dropout=0.3,recurrent_dropout=0.3)\n",
    "        decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state = [state_h,state_c])\n",
    "        \n",
    "        # Attention\n",
    "        attn_layer = AttentionLayer(name='attention_layer')\n",
    "        attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n",
    "        \n",
    "        # Concatenate the context vectors with the decoder outpouts\n",
    "        decoder_concat = Concatenate()([decoder_outputs, attn_out])\n",
    "        \n",
    "        # Dense\n",
    "        decoder_dense =  TimeDistributed(Dense(y_vocab_size, activation='softmax'))\n",
    "        decoder_outputs = decoder_dense(decoder_concat)\n",
    "        \n",
    "        # model\n",
    "        trainer_model = Model(inputs=[encoder_inputs,decoder_inputs],outputs=decoder_outputs)\n",
    "        \n",
    "        \n",
    "        \"\"\"Inference Phase\"\"\"\n",
    "        # Encoder\n",
    "        encoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n",
    "        \n",
    "        # Decoder\n",
    "        decoder_state_input_h = Input(shape=(2* self.latent_dim,))\n",
    "        decoder_state_input_c = Input(shape=(2* self.latent_dim,))\n",
    "        decoder_hidden_state_input = Input(shape=(max_length_text, 2* self.latent_dim))\n",
    "        dec_emb2 = dec_emb_layer(decoder_inputs)\n",
    "        decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state = [decoder_state_input_h,decoder_state_input_c])\n",
    "\n",
    "        # Attention \n",
    "        attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
    "        decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
    "        \n",
    "        # Dense\n",
    "        decoder_outputs2 = decoder_dense(decoder_inf_concat)\n",
    "        decoder_model = Model([decoder_inputs] + [decoder_hidden_state_input, decoder_state_input_h,decoder_state_input_c], \n",
    "                              [decoder_outputs2] + [state_h2, state_c2])\n",
    "        \n",
    "        \n",
    "        return trainer_model, encoder_model, decoder_model\n",
    "    \n",
    "    \n",
    "    def compile_model(self,model,optimizer='adam',loss='sparse_categorical_crossentropy'):\n",
    "        model.compile(optimizer, loss)\n",
    "        \n",
    "    def train_model(self,model,x_tr,x_dev,y_tr,y_dev,epochs=50,batch_size=128):\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\n",
    "        history=model.fit([x_tr,y_tr[:,:-1]], y_tr.reshape(y_tr.shape[0],y_tr.shape[1], 1)[:,1:] ,epochs=epochs,callbacks=[es],batch_size=batch_size, validation_data=([x_dev,y_dev[:,:-1]], y_dev.reshape(y_dev.shape[0],y_dev.shape[1], 1)[:,1:]))\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def diagnostic_plot(self,history):\n",
    "        plt.plot(history.history['loss'], label='train')\n",
    "        plt.plot(history.history['val_loss'], label='dev')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "    def save_model(self,encoder_model,decoder_model):\n",
    "        with open('encoder_model.json', 'w', encoding='utf8') as f:\n",
    "            f.write(encoder_model.to_json())\n",
    "        encoder_model.save_weights('encoder_model_weights.h5')\n",
    "\n",
    "        with open('decoder_model.json', 'w', encoding='utf8') as f:\n",
    "            f.write(decoder_model.to_json())\n",
    "        decoder_model.save_weights('decoder_model_weights.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
